{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dntfreitas/2023-prompt-engineering-code/blob/main/open-ai-api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "a15b7ec8bbbadd30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A word about the transformer's base function Word2Vec\n",
    "\n",
    "Let's start this notebook by looking at the base function of the Transformer model Word2Vec.\n",
    "\n",
    "We will show how to train a Word2Vec model to be used as an input embedding layer for a Transformer model.\n",
    "\n",
    "The outcome of the Word2Vec is a collection of word vectors, with vectors that are closely positioned in vector space signifying similar meanings derived from their context, while word vectors situated far apart indicate contrasting meanings.\n",
    "\n",
    "The intuition behind this model is that words that appear in similar contexts are semantically similar."
   ],
   "metadata": {
    "collapsed": false,
    "id": "1187a90c86fd26ad"
   },
   "id": "1187a90c86fd26ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by importing the required libraries and load the data."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ed04d5663494720c"
   },
   "id": "ed04d5663494720c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:09.879624Z",
     "start_time": "2023-09-10T15:55:08.244565Z"
    },
    "id": "d82cc66bb27edfbc"
   },
   "id": "d82cc66bb27edfbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('imdb.csv')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:10.114907Z",
     "start_time": "2023-09-10T15:55:09.880293Z"
    },
    "id": "c503ff61948834e1"
   },
   "id": "c503ff61948834e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization and Stopword Removal"
   ],
   "metadata": {
    "collapsed": false,
    "id": "2745a5fedd4418d7"
   },
   "id": "2745a5fedd4418d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will start by tokenize the text into words, which is required for training the Word2Vec model.\n",
    "\n",
    "After that, it is recommended to remove stopwords and pontuation from the text before training the Word2Vec model. Stopwords are words that appear very frequently in the English language, such as \"the\", \"a\", \"an\", etc.\n",
    "These words do not add much value to the model, so it is recommended to remove them.\n",
    "\n",
    "After removing the stopwords, we will lemmatize the words. Lemmatization is the process of converting a word to its base form. For example, the word \"running\" will be converted to \"run\". This is done to reduce the number of unique words in the corpus, which will reduce the size of the Word2Vec model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "3c24c1942929cd4f"
   },
   "id": "3c24c1942929cd4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text and remove stopwords\n",
    "    :param text: the text to process\n",
    "    :return: the processed text tokenized and without stopwords\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in tokenized_text if word not in stop_words]\n",
    "    # Remove punctuation\n",
    "    filtered_text = [word for word in filtered_text if word.isalpha()]\n",
    "    # Lemmatize words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    filtered_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    return filtered_text"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:10.119890Z",
     "start_time": "2023-09-10T15:55:10.115883Z"
    },
    "id": "cf2e434af8c8455"
   },
   "id": "cf2e434af8c8455"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply the function to the corpus to get a list of tokenized reviews\n",
    "tokenized_reviews = [process_text(text) for text in corpus['Review']]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:25.583967Z",
     "start_time": "2023-09-10T15:55:10.118455Z"
    },
    "id": "e121fc4031cc19a0"
   },
   "id": "e121fc4031cc19a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Word2Vec Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "699a698c4b2978d7"
   },
   "id": "699a698c4b2978d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the Word2Vec model using the tokenized reviews and the training algorithm CBOW (Continuous Bag of Words).\n",
    "# The CBOW model architecture aims to forecast the central word (the target word) by relying on the context words from the surrounding text. To illustrate this concept using a basic sentence, such as \"the quick brown fox jumps over the lazy dog,\" we can form pairs of (context window, target word). When we adopt a context window size of 2, these pairs might appear as follows: ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy), and so forth. Consequently, the model goal is to make predictions for the target word using the context window words as input.\n",
    "model = Word2Vec(tokenized_reviews, window=5, sg=0)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:32.728174Z",
     "start_time": "2023-09-10T15:55:25.627301Z"
    },
    "id": "48254b0e333f8d86"
   },
   "id": "48254b0e333f8d86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the vector for a word\n",
    "word = 'shoot'\n",
    "\n",
    "# Get the vector for a word\n",
    "word_vector = model.wv[word]\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(word, topn=5)\n",
    "\n",
    "# Print the word vector and similar words\n",
    "print(f\"Vector for '{word}': {word_vector}\")\n",
    "print(f\"Similar words to '{word}': {similar_words}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:32.793789Z",
     "start_time": "2023-09-10T15:55:32.728592Z"
    },
    "id": "82a0a1e63b315092"
   },
   "id": "82a0a1e63b315092"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OpenAI API\n",
    "\n",
    "The rest of the notebook will be dedicated to the OpenAI API. We will show how to use the API to sentiment analysis and summarization.\n",
    "\n",
    "Let's start by installing and importing the OpenAI library, as well as the other libraries required for this part of the notebook"
   ],
   "metadata": {
    "collapsed": false,
    "id": "7dc525129e8beda5"
   },
   "id": "7dc525129e8beda5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv\n",
    "!pip install pypdf"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.290121Z",
     "start_time": "2023-09-10T15:55:32.783882Z"
    },
    "id": "57f1c43a6d36c043"
   },
   "id": "57f1c43a6d36c043"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.372026Z",
     "start_time": "2023-09-10T15:55:35.290729Z"
    },
    "id": "68364e8635d35727"
   },
   "id": "68364e8635d35727"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Completion function\n",
    "\n",
    "The follwing function will be used to get the completion from the OpenAI API."
   ],
   "metadata": {
    "collapsed": false,
    "id": "aeb876d231cd973e"
   },
   "id": "aeb876d231cd973e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    \"\"\"\n",
    "    Get the completion from the OpenAI API\n",
    "    :param prompt: the prompt to send to the API\n",
    "    :param model: the model to use\n",
    "    :param temperature: the temperature to use\n",
    "    :return: the completion\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    completion = response.choices[0].message[\"content\"]\n",
    "    return completion"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.376150Z",
     "start_time": "2023-09-10T15:55:35.372930Z"
    },
    "id": "eca7b8e9f4f7e811"
   },
   "id": "eca7b8e9f4f7e811"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration of the OpenAI API\n",
    "\n",
    "Before we can use the API, we need to configure it with our API key. The API key will be stored in a file called `.env`, which will be loaded by the `dotenv` library and used to configure the API via environment variables.\n",
    "\n",
    "The API key can be found in the OpenAI dashboard."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c12ea1e3df9760f9"
   },
   "id": "c12ea1e3df9760f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.379997Z",
     "start_time": "2023-09-10T15:55:35.375158Z"
    },
    "id": "3c9829df90c2c56e"
   },
   "id": "3c9829df90c2c56e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = load_dotenv('.env')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.383683Z",
     "start_time": "2023-09-10T15:55:35.380195Z"
    },
    "id": "ba5d84b4fa0c5d30"
   },
   "id": "ba5d84b4fa0c5d30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.384534Z",
     "start_time": "2023-09-10T15:55:35.382501Z"
    },
    "id": "caca4663447bc0ca"
   },
   "id": "caca4663447bc0ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Optional:_ Define a hellper function to print contents in markdown format."
   ],
   "metadata": {
    "collapsed": false,
    "id": "272b8777a58caaa8"
   },
   "id": "272b8777a58caaa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.387280Z",
     "start_time": "2023-09-10T15:55:35.384696Z"
    },
    "id": "65ef4141c616fe21"
   },
   "id": "65ef4141c616fe21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    \"\"\"\n",
    "    Print markdown content in the notebook.\n",
    "    :param string: the markdown content\n",
    "    \"\"\"\n",
    "    display(Markdown(string))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:35.390484Z",
     "start_time": "2023-09-10T15:55:35.389019Z"
    },
    "id": "37377fc9061c1b18"
   },
   "id": "37377fc9061c1b18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can test our API configuration by sending a prompt to the API and printing the completion."
   ],
   "metadata": {
    "collapsed": false,
    "id": "3fbd51f3c402aa57"
   },
   "id": "3fbd51f3c402aa57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "printmd(get_completion(\"Craft a concise invitation for the AI Summit on 13-09-2023 at 2pm at the Museu da Imprensa da Madeira, keeping in mind that the audience will be tech professionals, researchers, and enthusiasts, and the format should be in a standard invitation style.\"))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:42.416046Z",
     "start_time": "2023-09-10T15:55:35.391014Z"
    },
    "id": "7ccc602607bb4d49"
   },
   "id": "7ccc602607bb4d49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Now that we have configured the API, we can start using it to perform several tasks.**"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4ab2cfab0d75598e"
   },
   "id": "4ab2cfab0d75598e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "The following function will be used to perform sentiment analysis using the OpenAI API. Sentiment analysis is the process of determining whether a piece of writing is positive, negative, or neutral."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e81df65e6315be59"
   },
   "id": "e81df65e6315be59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    prompt = f\"What is the sentiment of the following movie review, which is delimited with triple backticks? Give your answer as a single word, either 'positive', 'negative' or 'neutral'. Review text: ```{text}```\"\n",
    "    return get_completion(prompt)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:42.424832Z",
     "start_time": "2023-09-10T15:55:42.416748Z"
    },
    "id": "d77828b68e0f57a3"
   },
   "id": "d77828b68e0f57a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's test the function with a few reviews from the corpus\n",
    "for i in range(10):\n",
    "    print(f\"Review {i}: {corpus.iloc[i]['Review']}--> Sentiment: {get_sentiment(corpus.iloc[i]['Review'])} \\n\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:48.542022Z",
     "start_time": "2023-09-10T15:55:42.421489Z"
    },
    "id": "309294fe0d4c2f26"
   },
   "id": "309294fe0d4c2f26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inferring Topics\n",
    "\n",
    "Is the task of inferring topics from a piece of text."
   ],
   "metadata": {
    "collapsed": false,
    "id": "aa0567f080de56fd"
   },
   "id": "aa0567f080de56fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the suggestions from the text file\n",
    "with open('desired-topics.txt', 'r') as f:\n",
    "    suggestions = f.read()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:48.548927Z",
     "start_time": "2023-09-10T15:55:48.543771Z"
    },
    "id": "ca8d4e53bb7c4d39"
   },
   "id": "ca8d4e53bb7c4d39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine five topics that the participants of the MML '23 want to see discussed in the event. The suggestions are delimited by triple backticks. Each line corresponds to a user suggestion.\n",
    "Make each item one or two words long.\n",
    "Format your response as a list of items separated by commas.\n",
    "\n",
    "Suggestions: '''{suggestions}'''\n",
    "\"\"\"\n",
    "\n",
    "printmd(get_completion(prompt))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:49.582481Z",
     "start_time": "2023-09-10T15:55:48.548481Z"
    },
    "id": "29298261006694aa"
   },
   "id": "29298261006694aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarization and Q&A\n",
    "\n",
    "The follwing section will read a PDF file and summarize it using the OpenAI API. Also, it will answer a few questions about the text."
   ],
   "metadata": {
    "collapsed": false,
    "id": "fcaf9736d75b2067"
   },
   "id": "fcaf9736d75b2067"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pypdf import PdfReader"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:55:49.623372Z",
     "start_time": "2023-09-10T15:55:49.583543Z"
    },
    "id": "f182de188b24481c"
   },
   "id": "f182de188b24481c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the PDF file and extract the text\n",
    "text = \"\"\n",
    "\n",
    "reader = PdfReader(\"regulation.pdf\")\n",
    "\n",
    "for i in range(len(reader.pages)):\n",
    "  page = reader.pages[i]\n",
    "  text += page.extract_text()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:56:31.111955Z",
     "start_time": "2023-09-10T15:56:30.465113Z"
    },
    "id": "84c3ffb414041e4b"
   },
   "id": "84c3ffb414041e4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Your task is to perform the following actions:\n",
    "1 - Summarize the following text delimited by <> in at most 100 words. Focus on eligibility criteria, application process, and evaluation criteria.\n",
    "2 - Translate the summary into French.\n",
    "Text: <{text}>\"\"\"\n",
    "\n",
    "printmd(get_completion(prompt))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:56:01.763970Z",
     "start_time": "2023-09-10T15:55:49.636460Z"
    },
    "id": "6f32c6c501a27645"
   },
   "id": "6f32c6c501a27645"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Your task is to answer the following questions, based on text delimited by <>:\n",
    "1 - What is the eligibility criteria for the funding?\n",
    "2 - What is the maximum amount of funding that can be requested?\n",
    "Text: <{text}>\"\"\"\n",
    "\n",
    "printmd(get_completion(prompt))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T15:56:04.393182Z",
     "start_time": "2023-09-10T15:56:01.766117Z"
    },
    "id": "5fea4d2cef7ac606"
   },
   "id": "5fea4d2cef7ac606"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
